{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f893cd-dbe1-404e-8642-39151b2745d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 백엔드를 설정하는 노트북을 초기화합니다. \n",
    "\n",
    "이 노트북에서는 헬퍼 함수들이 포함되어 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c99f8f-e9ac-4641-a57f-a5ad28c8b8df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만약에 스키마가 없다면 생성합니다.\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {uc_catalog}.{uc_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0136c7f-da13-480c-9a84-1c847728673b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 서비스 이름 설정\n",
    "email = spark.sql('select current_user() as user').collect()[0]['user']\n",
    "username = email.split('@')[0].replace('.', '_')\n",
    "user_name = username\n",
    "\n",
    "vector_search_endpoint_name = f\"vs_ep_{uc_catalog}_{uc_schema}\"\n",
    "serving_endpoint_name = f\"ser_ep_{uc_catalog}_{uc_schema}\"\n",
    "\n",
    "print(f\"Vector Search endpoint name: {vector_search_endpoint_name}\")\n",
    "print(f\"Serving endpoint name: {serving_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "445c830f-01c0-48a4-ae5c-aab819d12884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"reset_all_data\", \"false\", \"Reset Data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c9eedc1-fda4-4c86-a157-1a97510529f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf\n",
    "import os\n",
    "import mlflow\n",
    "from typing import Iterator\n",
    "from mlflow import MlflowClient\n",
    "from databricks.vector_search.client import VectorSearchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce795ed-c2cc-4a0b-b73c-0855a8a9353e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "min_required_version = \"11.3\"\n",
    "version_tag = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "version_search = re.search('^([0-9]*\\.[0-9]*)', version_tag)\n",
    "assert version_search, f\"The Databricks version can't be extracted from {version_tag}, shouldn't happen, please correct the regex\"\n",
    "current_version = float(version_search.group(1))\n",
    "assert float(current_version) >= float(min_required_version), f'The Databricks version of the cluster must be >= {min_required_version}. Current version detected: {current_version}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e2144c0-da95-4bc3-abae-cfa61c0f147f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if reset_all_data:\n",
    "  print(f'clearing up db {dbName}')\n",
    "  spark.sql(f\"DROP DATABASE IF EXISTS `{dbName}` CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe99d7d8-3d0a-48ae-951a-2ccebe05e8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CATALOG `insungcatalog`\nusing catalog.database `insungcatalog`.`insungschema`\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "assert catalog not in ['hive_metastore', 'spark_catalog']\n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs:\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "      if catalog == 'dbdemos':\n",
    "        spark.sql(f\"ALTER CATALOG {catalog} OWNER TO `account users`\")\n",
    "  use_and_create_db(catalog, dbName)\n",
    "\n",
    "if catalog == 'dbdemos':\n",
    "  try:\n",
    "    spark.sql(f\"GRANT CREATE, USAGE on DATABASE {catalog}.{dbName} TO `account users`\")\n",
    "    spark.sql(f\"ALTER SCHEMA {catalog}.{dbName} OWNER TO `account users`\")\n",
    "  except Exception as e:\n",
    "    print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "print(f\"using catalog.database `{catalog}`.`{dbName}`\")\n",
    "spark.sql(f\"\"\"USE `{catalog}`.`{dbName}`\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c11a271-11ad-4f12-b60f-60bea48a1906",
     "showTitle": true,
     "title": "Optional: Allowing Model Serving IPs"
    }
   },
   "outputs": [],
   "source": [
    "#If your workspace has ip access list, you need to allow your model serving endpoint to hit your AI gateway. Based on your region, IPs might change. Please reach out your Databrics Account team for more details.\n",
    "\n",
    "# def allow_serverless_ip():\n",
    "#   base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get(),\n",
    "#   headers = {\"Authorization\": f\"Bearer {<Your PAT Token>}\", \"Content-Type\": \"application/json\"}\n",
    "#   return requests.post(f\"{base_url}/api/2.0/ip-access-lists\", json={\"label\": \"serverless-model-serving\", \"list_type\": \"ALLOW\", \"ip_addresses\": [\"<IP RANGE>\"], \"enabled\": \"true\"}, headers = headers).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d883ef-8ca6-4847-b754-2f96b8d0750e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 카탈로그 및 인덱스 상태를 가져오는 헬퍼입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f50d93-8deb-4541-9b37-22b9059c68c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_latest_model_version(model_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501d8439-0d20-4c2e-b4a0-2353487c6f92",
     "showTitle": true,
     "title": "endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dfe002f-f2fa-42ec-85a6-442ad7b9a4f2",
     "showTitle": true,
     "title": "index"
    }
   },
   "outputs": [],
   "source": [
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "            raise e\n",
    "    return False\n",
    "    \n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c764a159-5076-4a94-9273-bbb171971c03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StringType\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "#Add retries with backoff to avoid 429 while fetching the doc\n",
    "retries = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=3,\n",
    "    status_forcelist=[429],\n",
    ")\n",
    "\n",
    "def download_databricks_documentation_articles(max_documents=None):\n",
    "    # Fetch the XML content from sitemap\n",
    "    response = requests.get(DATABRICKS_SITEMAP_URL)\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    # Find all 'loc' elements (URLs) in the XML\n",
    "    urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
    "    if max_documents:\n",
    "        urls = urls[:max_documents]\n",
    "\n",
    "    # Create DataFrame from URLs\n",
    "    df_urls = spark.createDataFrame(urls, StringType()).toDF(\"url\").repartition(10)\n",
    "\n",
    "    # Pandas UDF to fetch HTML content for a batch of URLs\n",
    "    @pandas_udf(\"string\")\n",
    "    def fetch_html_udf(urls: pd.Series) -> pd.Series:\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        http = requests.Session()\n",
    "        http.mount(\"http://\", adapter)\n",
    "        http.mount(\"https://\", adapter)\n",
    "        def fetch_html(url):\n",
    "            try:\n",
    "                response = http.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.content\n",
    "            except requests.RequestException:\n",
    "                return None\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "            results = list(executor.map(fetch_html, urls))\n",
    "        return pd.Series(results)\n",
    "\n",
    "    # Pandas UDF to process HTML content and extract text\n",
    "    @pandas_udf(\"string\")\n",
    "    def download_web_page_udf(html_contents: pd.Series) -> pd.Series:\n",
    "        def extract_text(html_content):\n",
    "            if html_content:\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                article_div = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "                if article_div:\n",
    "                    return str(article_div).strip()\n",
    "            return None\n",
    "\n",
    "        return html_contents.apply(extract_text)\n",
    "\n",
    "    # Apply UDFs to DataFrame\n",
    "    df_with_html = df_urls.withColumn(\"html_content\", fetch_html_udf(\"url\"))\n",
    "    final_df = df_with_html.withColumn(\"text\", download_web_page_udf(\"html_content\"))\n",
    "\n",
    "    # Select and filter non-null results\n",
    "    final_df = final_df.select(\"url\", \"text\").filter(\"text IS NOT NULL\").cache()\n",
    "    if final_df.isEmpty():\n",
    "      raise Exception(\"Dataframe is empty, couldn't download Databricks documentation, please check sitemap status.\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e231ac5-fc18-42a0-bbc3-deef3156cae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_gradio_app(space_name = \"databricks-demos-chatbot\"):\n",
    "    displayHTML(f'''<div style=\"margin: auto; width: 1000px\"><iframe src=\"https://{space_name}.hf.space\" frameborder=\"0\" width=\"1000\" height=\"950\" style=\"margin: auto\"></iframe></div>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4310db44-3d6f-4f7f-a1e4-4d51fe7e4fbb",
     "showTitle": true,
     "title": "Cleanup utility to remove demo assets"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_demo(catalog, db, serving_endpoint_name, vs_index_fullname):\n",
    "  vsc = VectorSearchClient()\n",
    "  try:\n",
    "    vsc.delete_index(endpoint_name = vector_search_endpoint_name, index_name=vs_index_fullname)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete index {vector_search_endpoint_name} {vs_index_fullname} - might not be existing: {e}\")\n",
    "  try:\n",
    "    WorkspaceClient().serving_endpoints.delete(serving_endpoint_name)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete serving endpoint {serving_endpoint_name} - might not be existing: {e}\")\n",
    "  spark.sql(f'DROP SCHEMA `{catalog}`.`{db}` CASCADE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14158244-667d-4859-b260-13c4e69b074d",
     "showTitle": true,
     "title": "Demo helper to debug permission issue"
    }
   },
   "outputs": [],
   "source": [
    "def test_demo_permissions(host, secret_scope, secret_key, vs_endpoint_name, index_name, embedding_endpoint_name = None, managed_embeddings = True):\n",
    "  error = False\n",
    "  CSS_REPORT = \"\"\"\n",
    "  <style>\n",
    "  .dbdemos_install{\n",
    "                      font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji,FontAwesome;\n",
    "  color: #3b3b3b;\n",
    "  box-shadow: 0 .15rem 1.15rem 0 rgba(58,59,69,.15)!important;\n",
    "  padding: 10px 20px 20px 20px;\n",
    "  margin: 10px;\n",
    "  font-size: 14px !important;\n",
    "  }\n",
    "  .dbdemos_block{\n",
    "      display: block !important;\n",
    "      width: 900px;\n",
    "  }\n",
    "  .code {\n",
    "      padding: 5px;\n",
    "      border: 1px solid #e4e4e4;\n",
    "      font-family: monospace;\n",
    "      background-color: #f5f5f5;\n",
    "      margin: 5px 0px 0px 0px;\n",
    "      display: inline;\n",
    "  }\n",
    "  </style>\"\"\"\n",
    "\n",
    "  def display_error(title, error, color=\"\"):\n",
    "    displayHTML(f\"\"\"{CSS_REPORT}\n",
    "      <div class=\"dbdemos_install\">\n",
    "                          <h1 style=\"color: #eb0707\">Configuration error: {title}</h1> \n",
    "                            {error}\n",
    "                        </div>\"\"\")\n",
    "  \n",
    "  def get_email():\n",
    "    try:\n",
    "      return spark.sql('select current_user() as user').collect()[0]['user']\n",
    "    except:\n",
    "      return 'Uknown'\n",
    "\n",
    "  def get_token_error(msg, e):\n",
    "    return f\"\"\"\n",
    "    {msg}<br/><br/>\n",
    "    Your model will be served using Databrick Serverless endpoint and needs a Pat Token to authenticate.<br/>\n",
    "    <strong> This must be saved as a secret to be accessible when the model is deployed.</strong><br/><br/>\n",
    "    Here is how you can add the Pat Token as a secret available within your notebook and for the model:\n",
    "    <ul>\n",
    "    <li>\n",
    "      first, setup the Databricks CLI on your laptop or using this cluster terminal:\n",
    "      <div class=\"code dbdemos_block\">pip install databricks-cli</div>\n",
    "    </li>\n",
    "    <li> \n",
    "      Configure the CLI. You'll need your workspace URL and a PAT token from your profile page\n",
    "      <div class=\"code dbdemos_block\">databricks configure</div>\n",
    "    </li>  \n",
    "    <li>\n",
    "      Create the dbdemos scope:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets create-scope dbdemos</div>\n",
    "    <li>\n",
    "      Save your service principal secret. It will be used by the Model Endpoint to autenticate. <br/>\n",
    "      If this is a demo/test, you can use one of your PAT token.\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-secret dbdemos rag_sp_token</div>\n",
    "    </li>\n",
    "    <li>\n",
    "      Optional - if someone else created the scope, make sure they give you read access to the secret:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-acl dbdemos '{get_email()}' READ</div>\n",
    "\n",
    "    </li>  \n",
    "    </ul>  \n",
    "    <br/>\n",
    "    Detailed error trying to access the secret:\n",
    "      <div class=\"code dbdemos_block\">{e}</div>\"\"\"\n",
    "\n",
    "  try:\n",
    "    secret = dbutils.secrets.get(secret_scope, secret_key)\n",
    "    secret_principal = \"__UNKNOWN__\"\n",
    "    try:\n",
    "      from databricks.sdk import WorkspaceClient\n",
    "      w = WorkspaceClient(token=dbutils.secrets.get(secret_scope, secret_key), host=host)\n",
    "      secret_principal = w.current_user.me().emails[0].value\n",
    "    except Exception as e_sp:\n",
    "      error = True\n",
    "      display_error(f\"Couldn't get the SP identity using the Pat Token saved in your secret\", \n",
    "                    get_token_error(f\"<strong>This likely means that the Pat Token saved in your secret {secret_scope}/{secret_key} is incorrect or expired. Consider replacing it.</strong>\", e_sp))\n",
    "      return\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    display_error(f\"We couldn't access the Pat Token saved in the secret {secret_scope}/{secret_key}\", \n",
    "                  get_token_error(\"<strong>This likely means your secret isn't set or not accessible for your user</strong>.\", e))\n",
    "    return\n",
    "  \n",
    "  try:\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=secret, disable_notice=True)\n",
    "    vs_index = vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name)\n",
    "    if embedding_endpoint_name:\n",
    "      if managed_embeddings:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        results = vs_index.similarity_search(query_text='What is Apache Spark?', columns=[\"content\"], num_results=1)\n",
    "      else:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        embedding_model = DatabricksEmbeddings(endpoint=embedding_endpoint_name)\n",
    "        embeddings = embedding_model.embed_query('What is Apache Spark?')\n",
    "        results = vs_index.similarity_search(query_vector=embeddings, columns=[\"content\"], num_results=1)\n",
    "\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    vs_error = f\"\"\"\n",
    "    Why are we getting this error?<br/>\n",
    "    The model is using the Pat Token saved with the secret {secret_scope}/{secret_key} to access your vector search index '{index_name}' (host:{host}).<br/><br/>\n",
    "    To do so, the principal owning the Pat Token must have USAGE permission on your schema and READ permission on the index.<br/>\n",
    "    The principal is the one who generated the token you saved as secret: `{secret_principal}`. <br/>\n",
    "    <i>Note: Production-grade deployement should to use a Service Principal ID instead.</i><br/>\n",
    "    <br/>\n",
    "    Here is how you can fix it:<br/><br/>\n",
    "    <strong>Make sure your Service Principal has USE privileve on the schema</strong>:\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    spark.sql('GRANT USAGE ON CATALOG `{catalog}` TO `{secret_principal}`');<br/>\n",
    "    spark.sql('GRANT USAGE ON DATABASE `{catalog}`.`{db}` TO `{secret_principal}`');<br/>\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>Grant SELECT access to your SP to your index:</strong>\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    from databricks.sdk import WorkspaceClient<br/>\n",
    "    import databricks.sdk.service.catalog as c<br/>\n",
    "    WorkspaceClient().grants.update(c.SecurableType.TABLE, \"{index_name}\",<br/>\n",
    "                                            changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"{secret_principal}\")])\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>If this is still not working, make sure the value saved in your {secret_scope}/{secret_key} secret is your SP pat token </strong>.<br/>\n",
    "    <i>Note: if you're using a shared demo workspace, please do not change the secret value if was set to a valid SP value by your admins.</i>\n",
    "\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Detailed error trying to access the endpoint:\n",
    "    <div class=\"code dbdemos_block\">{str(e)}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    if \"403\" in str(e):\n",
    "      display_error(f\"Permission error on Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "    else:\n",
    "      display_error(f\"Unkown error accessing the Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "  def get_wid():\n",
    "    try:\n",
    "      return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('orgId')\n",
    "    except:\n",
    "      return None\n",
    "  if get_wid() in [\"5206439413157315\", \"984752964297111\", \"1444828305810485\", \"2556758628403379\"]:\n",
    "    print(f\"----------------------------\\nYou are in a Shared FE workspace. Please don't override the secret value (it's set to the SP `{secret_principal}`).\\n---------------------------\")\n",
    "\n",
    "  if not error:\n",
    "    print('Secret and permissions seems to be properly setup, you can continue the demo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d76654a-bee4-4701-9ce0-34c20419b4ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pprint(obj):\n",
    "  import pprint\n",
    "  pprint.pprint(obj, compact=True, indent=1, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c338467-6dcd-46b7-85d2-f0e9d083952b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Temp workaround to test if a table exists in shared cluster mode in DBR 14.2 (see SASP-2467)\n",
    "def table_exists(table_name):\n",
    "    try:\n",
    "        spark.table(table_name).isEmpty()\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2530113032448329,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "init-script",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
